{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d683e33-389a-456e-bed6-1fed93fb5167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2361a-02df-46a4-8a00-58f13adf127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = r''\n",
    "dbSQL = ''\n",
    "user = ''\n",
    "pwd = ''\n",
    "MC0 = '''ABS AH- ALM BC- BL- CA- CAE CHG COM CU- FP- FW- GA- GBL GPP GUE HVC LBU LGR LIT MWT PLT PND PRD PWS RS- SCR SD- SDR SPP STB SW- TIC TRF UDT WST'''.split(' ')\n",
    "MC1 = '''ABS AF- AH- ALM BC- BL- CA- CAE CBA CCW CHG CND COM CU- ESP EXC FAT FG- FP- FW- GA- GUE GYP HVC LIT MWT PFC PJF PLT PRD RS- BCW SD- SDR SIS STB SW- TA- TIC TRF'''.split(' ')\n",
    "MC2 = '''ABS AF- AH- ALM BC- BL- CA- CAE CBA CCW CHG CND COM CU- ESP EXC FAT FG- FP- FW- GA- GUE GYP HVC LIT MWT PFC PJF PLT PRD RS- BCW SD- SDR SIS STB SW- TA- TIC TRF'''.split(' ')\n",
    "MC3 = '''ABS AF- AH- ALM BC- BL- CA- CAE CBA CCW CHG CND COM CU- ESP EXC FAT FG- FP- FW- GA- GUE GYP HVC LIT MWT PFC PJF PLT PRD RS- SCR SD- SDR SIS STB SW- TA- TIC TRF'''.split(' ')\n",
    "#Consider adding MC00X in fron to all above in code os I don't need to do it later in teh code :)\n",
    "#I will do this in an algorithm later by pulling in MaximoDW.dbo.Locations and grabbing DISTINCT SUBSTRING(LOCATION, 4, 8) but right now i'm lazy\n",
    "where = ''\n",
    "for l in MC0: where += f'OR LOCATION LIKE \\'MC000{l}%\\' '\n",
    "for l in MC1: where += f'OR LOCATION LIKE \\'MC001{l}%\\' '\n",
    "for l in MC2: where += f'OR LOCATION LIKE \\'MC002{l}%\\' '\n",
    "for l in MC3: where += f'OR LOCATION LIKE \\'MC003{l}%\\' '\n",
    "where = where[3:]\n",
    "maximoSelect = f\"\"\"SELECT [\n",
    "\n",
    "\"\"\"\n",
    "woSelect= f\"\"\"SELECT [WONUM]\n",
    "      ,[LOCATION]\n",
    "      ,[DESCRIPTION]\n",
    "  FROM [Generation].[agent].[MaximoWorkOrders]\n",
    "  Where cast(STATUSDATE as date) > '01/01/2021'\"\"\"\n",
    " # And ({where})\n",
    "#getting data and then relableing maximo location to higher level equipment name\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+host+';DATABASE='+dbSQL+';UID='+user+';PWD='+ pwd)\n",
    "WOdata = pd.read_sql(woSelect, cnxn)\n",
    "#for key in MC0: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC000' + key)] = 'MC000' + MC0[MC0.index(key)]\n",
    "#for key in MC1: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC001' + key)] = 'MC001' + MC1[MC1.index(key)]\n",
    "#for key in MC2: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC002' + key)] = 'MC002' + MC2[MC2.index(key)]\n",
    "#for key in MC3: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC003' + key)] = 'MC003' + MC3[MC3.index(key)]\n",
    "WOdata = WOdata.drop(columns=['WONUM'])\n",
    "WOdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527a05f-68ad-4365-b1d3-7d1026dc3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WOdata['Edited1'] = [list() for x in range(len(WOdata.index))]\n",
    "WOdata['Edited2'] = [list() for x in range(len(WOdata.index))]\n",
    "\n",
    "def processText(text):\n",
    "    #text_tokens = word_tokenize(text)\n",
    "    text_tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    text_tokens = [word for word in text_tokens if not word in stop_words]  ##Remove Stop Words\n",
    "    text_tokens = [word.lower() for word in text_tokens]\n",
    "    #text_tokens = [ps.stem(word) for word in text_tokens] #Stem Words\n",
    "    #text_tokens = [word for word in text_tokens if len(word) > 2] #Removing noise\n",
    "    #text_tokens = [word for word in text_tokens if word.isalpha()] #Remove Punctuation\n",
    "    #text_tokens = [ele for ele in text_tokens if ele.strip()] #remove empty spaces in list\n",
    "    return text_tokens\n",
    "#Applying Text Processing\n",
    "WOdata['Edited1'] = WOdata['DESCRIPTION'].apply(processText)\n",
    "WOdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07d0c27-d4a0-4808-9fd2-f1245cb4f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = WOdata['Edited1'].to_list()\n",
    "frequency = defaultdict(int)\n",
    "'''\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "'''\n",
    "dictionary = corpora.Dictionary(WOdata['Edited1'])\n",
    "#dictionary.filer_extremes()\n",
    "corpus = [dictionary.doc2bow(text) for text in WOdata['Edited1']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc6ac97-4307-4433-9035-5e04599d8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60e9e0d-f337-4142-be01-8fa32402cc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6507647 ['3', 'abs', 'agitator', 'oil']\n",
      "0.5617389 ['pm', 'inspect', 'abs', 'agitators']\n",
      "0.53442883 ['u4', 'abs', 'inlet', 'duct', 'inspection']\n",
      "0.5237368 ['4', '1', 'abs', 'building', 'sump', 'pump', 'repair']\n",
      "0.5151138 ['2', '1', 'a', 'abs', 'recycle', 'pump', 'rebuild']\n",
      "0.5137556 ['u4', 'abs', 'outlet', 'duct', 'inspection']\n",
      "0.5057857 ['pm', '14', '2', '1', 'abs', 'vent', 'valve', 'lube']\n",
      "0.5057857 ['pm', '14', '2', '1', 'abs', 'vent', 'valve', 'lube']\n",
      "0.5036441 ['pm', '12', '2', '1', 'abs', 'tower', 'transmitter', 'calibration', 'pm']\n",
      "0.5036441 ['pm', '12', '2', '1', 'abs', 'tower', 'transmitter', 'calibration', 'pm']\n",
      "0.5036441 ['pm', '12', '2', '1', 'abs', 'tower', 'transmitter', 'calibration', 'pm']\n",
      "0.50309664 ['pm', '14', '2', '3', 'abs', 'vent', 'valve', 'lube']\n",
      "0.50309664 ['pm', '14', '2', '3', 'abs', 'vent', 'valve', 'lube']\n",
      "0.50098884 ['pm', '12', '2', '3', 'abs', 'tower', 'transmitter', 'calibration', 'pm']\n",
      "0.50098884 ['pm', '12', '2', '3', 'abs', 'tower', 'transmitter', 'calibration', 'pm']\n",
      "0.49838752 ['abs', 'transformer', 'fire', 'system', 'in', 'alarm']\n",
      "0.48786485 ['pm', '14', '2', '2', 'abs', 'vent', 'valve', 'lube']\n",
      "0.48786485 ['pm', '14', '2', '2', 'abs', 'vent', 'valve', 'lube']\n",
      "0.4860496 ['0', '5', 'abs', 'agitator', 'motor', 'leaking', 'oil']\n",
      "0.48594207 ['pm', '12', '2', '2', 'abs', 'tower', 'transmitter', 'calibration', 'pm']\n",
      "0.48594207 ['pm', '12', '2', '2', 'abs', 'tower', 'transmitter', 'calibration', 'pm']\n"
     ]
    }
   ],
   "source": [
    "input = 'abs'\n",
    "vec_bow = dictionary.doc2bow(input.lower().split(' '))\n",
    "vec_lsi = tfidf[vec_bow]  # convert the query to LSI space\n",
    "#print(vec_lsi)\n",
    "#index = similarities.MatrixSimilarity(corpus) \n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "\n",
    "#print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -1 * item[1])\n",
    "x = 0\n",
    "for doc_position, doc_score in sims:\n",
    "    print(doc_score, WOdata['Edited1'][doc_position])\n",
    "    x += 1\n",
    "    if (x > 20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4bf72-9da4-4650-96f6-f7a5fd123835",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = models.Doc2Vec('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
