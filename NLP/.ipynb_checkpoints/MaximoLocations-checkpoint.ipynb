{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e105da81-b628-4d45-b99f-28a5e9002078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612e471-de94-4a1e-a03f-ba5f57483c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = r''\n",
    "dbSQL = ''\n",
    "user = ''\n",
    "pwd = ''\n",
    "MC0 = '''ABS AH- ALM BC- BL- CA- CAE CHG COM CU- FP- FW- GA- GBL GPP GUE HVC LBU LGR LIT MWT PLT PND PRD PWS RS- SCR SD- SDR SPP STB SW- TIC TRF UDT WST'''.split(' ')\n",
    "MC1 = '''ABS AF- AH- ALM BC- BL- CA- CAE CBA CCW CHG CND COM CU- ESP EXC FAT FG- FP- FW- GA- GUE GYP HVC LIT MWT PFC PJF PLT PRD RS- BCW SD- SDR SIS STB SW- TA- TIC TRF'''.split(' ')\n",
    "MC2 = '''ABS AF- AH- ALM BC- BL- CA- CAE CBA CCW CHG CND COM CU- ESP EXC FAT FG- FP- FW- GA- GUE GYP HVC LIT MWT PFC PJF PLT PRD RS- BCW SD- SDR SIS STB SW- TA- TIC TRF'''.split(' ')\n",
    "MC3 = '''ABS AF- AH- ALM BC- BL- CA- CAE CBA CCW CHG CND COM CU- ESP EXC FAT FG- FP- FW- GA- GUE GYP HVC LIT MWT PFC PJF PLT PRD RS- SCR SD- SDR SIS STB SW- TA- TIC TRF'''.split(' ')\n",
    "#Consider adding MC00X in fron to all above in code os I don't need to do it later in teh code :)\n",
    "#I will do this in an algorithm later by pulling in MaximoDW.dbo.Locations and grabbing DISTINCT SUBSTRING(LOCATION, 4, 8) but right now i'm lazy\n",
    "where = ''\n",
    "for l in MC0: where += f'OR LOCATION LIKE \\'MC000{l}%\\' '\n",
    "for l in MC1: where += f'OR LOCATION LIKE \\'MC001{l}%\\' '\n",
    "for l in MC2: where += f'OR LOCATION LIKE \\'MC002{l}%\\' '\n",
    "for l in MC3: where += f'OR LOCATION LIKE \\'MC003{l}%\\' '\n",
    "where = where[3:]\n",
    "maximoSelect = f\"\"\"SELECT [\n",
    "\n",
    "\"\"\"\n",
    "woSelect= f\"\"\"SELECT [WONUM]\n",
    "      ,[LOCATION]\n",
    "      ,[DESCRIPTION]\n",
    "  FROM [Generation].[agent].[MaximoWorkOrders]\n",
    "  Where LOC = 'MC'\n",
    "  and cast(STATUSDATE as date) > '01/01/2020'\n",
    "  And ({where})\"\"\"\n",
    "#getting data and then relableing maximo location to higher level equipment name\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+host+';DATABASE='+dbSQL+';UID='+user+';PWD='+ pwd)\n",
    "WOdata = pd.read_sql(woSelect, cnxn)\n",
    "for key in MC0: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC000' + key)] = 'MC000' + MC0[MC0.index(key)]\n",
    "for key in MC1: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC001' + key)] = 'MC001' + MC1[MC1.index(key)]\n",
    "for key in MC2: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC002' + key)] = 'MC002' + MC2[MC2.index(key)]\n",
    "for key in MC3: WOdata.LOCATION[WOdata.LOCATION.str.startswith('MC003' + key)] = 'MC003' + MC3[MC3.index(key)]\n",
    "WOdata = WOdata.drop(columns=['WONUM'])\n",
    "WOdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "cd9346c7-caf2-4e16-9030-e237f15576a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abs', 'absent']\n",
      "0.7129230551826468\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least one label specified must be in y_true",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-644-098c15bbba70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximoLocations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one label specified must be in y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: At least one label specified must be in y_true"
     ]
    }
   ],
   "source": [
    "##Lets try the Easy Way First\n",
    "y = WOdata['LOCATION']\n",
    "X = WOdata['DESCRIPTION']\n",
    "#Spliting Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=53)\n",
    "#processing stop_words\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "#training \"fit of curve\" sort of deal\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "#transforms our test data using the above fit.\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "print(count_vectorizer.get_feature_names()[1000:1002])\n",
    "#Classifier for words\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "print(metrics.accuracy_score(y_test,pred))\n",
    "cm = metrics.confusion_matrix(y_test,pred,labels = list(maximoLocations))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4b3d2-0745-422a-87c9-14167c8923eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing outputs for various text processors\n",
    "print(WOdata.DESCRIPTION[2])\n",
    "text_tokens = word_tokenize(WOdata.DESCRIPTION[2])\n",
    "#text_tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(WOdata.DESCRIPTION[16])\n",
    "print(text_tokens)\n",
    "text_tokens = [word for word in text_tokens if not word in stop_words]\n",
    "print(text_tokens)\n",
    "text_tokens = [ps.stem(word) for word in text_tokens] #Stem Words\n",
    "print(text_tokens)\n",
    "#May need to keep numbers later on;\n",
    "#text_tokens=[word for word in text_tokens if word.isalpha()]\n",
    "#print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "7af4e071-c7f2-4390-9ce7-a33843eaf044",
   "metadata": {},
   "outputs": [],
   "source": [
    "WOdata['Edited1'] = [list() for x in range(len(WOdata.index))]\n",
    "WOdata['Edited2'] = [list() for x in range(len(WOdata.index))]\n",
    "\n",
    "def processText(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    #text_tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    text_tokens = [word for word in text_tokens if not word in stop_words]  ##Remove Stop Words\n",
    "    text_tokens = [ps.stem(word) for word in text_tokens] #Stem Words\n",
    "    text_tokens = [word for word in text_tokens if len(word) > 2] #Removing noise\n",
    "    text_tokens = [word for word in text_tokens if word.isalpha()] #Remove Punctuation\n",
    "    text_tokens = [ele for ele in text_tokens if ele.strip()] #remove empty spaces in list\n",
    "    return text_tokens\n",
    "#Applying Text Processing\n",
    "WOdata['Edited1'] = WOdata['DESCRIPTION'].apply(processText)\n",
    "#Concating all the words in desc\n",
    "WOdata['Edited1Concat'] = WOdata['Edited1'].apply(lambda x: ' '.join(x))\n",
    "#WOdata.head\n",
    "#Concating all the strings into 1\n",
    "allWords = ' '.join(WOdata[\"Edited1Concat\"])\n",
    "allWords = word_tokenize(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96609e2-3e9f-4c58-8505-073ce369b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##POSSIBLY DROPPING WORDS BASED ON FREQUENCY OF USE\n",
    "allWordsDF = FreqDist(allWords)\n",
    "allWordsDF = pd.DataFrame(list(allWordsDF.items()), columns = [\"Word\",\"Frequency\"])\n",
    "allWordsDF = allWordsDF.sort_values(by=['Frequency'],ascending = False)\n",
    "#allWordsDF.hist(column=['Frequency'])\n",
    "#allWordsDF.hist(column=['Frequency'], range= (10,100))\n",
    "\n",
    "dictionary = allWordsDF.copy()\n",
    "dictionary = dictionary[dictionary['Frequency'] < 400]\n",
    "dictionary = dictionary[dictionary['Frequency'] > 3]\n",
    "#dictionary.hist(column=['Frequency'])\n",
    "dictionary = dictionary.reset_index(drop=True)\n",
    "selectedWords = dictionary.Word.tolist()\n",
    "\n",
    "\n",
    "for row in WOdata.index:\n",
    "    text_tokens = WOdata.Edited1[row]\n",
    "    text_tokens = [word for word in text_tokens if word in selectedWords]  ##Remove Stop Words\n",
    "    WOdata.Edited2[row]=text_tokens\n",
    "WOdata.Edited2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39779f7-1a0f-41b0-90f7-121e4c5149a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Edited2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Edited2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-626-d756bfdf42dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Test/Train Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWOdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LOCATION'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWOdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Edited2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstratify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Edited2'"
     ]
    }
   ],
   "source": [
    "##Test/Train Split\n",
    "y = WOdata['LOCATION'].values\n",
    "X = WOdata['Edited2'].apply(lambda x: ' '.join(x)).values\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 21,stratify = y)\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "print(count_vectorizer.get_feature_names()[:10])\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "print(metrics.accuracy_score(y_test,pred))\n",
    "cm = metrics.confusion_matrix(y_test,pred,labels = (maximoLocations))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "ac0414d4-9188-4dc7-961b-cc0445d508fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 0: \n",
      "KNN:  73.53695500116577\n",
      "CLS:  51.81860573560271\n",
      "BG:  59.955700629517374\n",
      "nb: 65.9944042900443\n",
      "\n",
      "knn avg: 0.7353695500116577\n",
      "cls avg: 0.5181860573560271\n",
      "bg avg: 0.5995570062951737\n",
      "nb avg: 0.659944042900443\n",
      "nn avg: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "##Create and Test Model\n",
    "#https://iq.opengenus.org/text-classification-using-k-nearest-neighbors/\n",
    "#\n",
    "#Testing kneighbors vs nb\n",
    "y = WOdata['LOCATION'].values\n",
    "X = WOdata['DESCRIPTION'].values\n",
    "#X = WOdata['Edited2'].apply(lambda x: ' '.join(x)).values\n",
    "#NOTES\n",
    "#KNN = 1 Seems to be best over KNN = 5 which is default\n",
    "#When using the post processed data, lose accuracy on KNN by .1-.25., and NB by .02-.03\n",
    "#\n",
    "knnAverage = 0\n",
    "clsAverage = 0\n",
    "bgAverage = 0\n",
    "nbAverage = 0\n",
    "nnAverage = 0\n",
    "runs = 1\n",
    "#run 1 run just to retrain model\n",
    "#just doing states 0-9, you can do whatever math to change those to be weirder.\n",
    "for run in range(runs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=run*6)\n",
    "    \n",
    "    #generic\n",
    "    count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "    #Tfidf is counting but also consider importance of wors and rare wrods.\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    #Training transofrmation\n",
    "    count_train = count_vectorizer.fit_transform(X_train)\n",
    "    X_count_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(count_train)\n",
    "    #testing transformation\n",
    "    X_new_counts = count_vectorizer.transform(X_test) #Test -> Counts\n",
    "    X_test_tfidf = tfidf_vectorizer.fit_transform(X_test)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts) #counts -> freq\n",
    "    \n",
    "    #knn\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "    knn=KNeighborsClassifier(n_neighbors = 5).fit(X_train_tfidf, y_train)\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', knn),\n",
    "    ])\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    predicted = text_clf.predict(X_test)\n",
    "    print(f'RUN {run}: ')\n",
    "    print(f'KNN: ',np.mean(predicted == y_test)*100)\n",
    "    knnAverage += np.mean(predicted == y_test)\n",
    "\n",
    "    \n",
    "    #cls\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "    cls = DecisionTreeClassifier().fit(X_train_tfidf, y_train)\n",
    "    predicted = cls.predict(X_new_counts)\n",
    "    print(f'CLS: ',np.mean(predicted == y_test)*100)\n",
    "    clsAverage += np.mean(predicted == y_test)\n",
    "    \n",
    "    #bg \n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "    bg = BaggingClassifier().fit(X_train_tfidf, y_train)\n",
    "    predicted = bg.predict(X_new_counts)\n",
    "    print(f'BG: ',np.mean(predicted == y_test)*100)\n",
    "    bgAverage += np.mean(predicted == y_test)\n",
    "    \n",
    "    #nb\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "    nb_classifier = MultinomialNB().fit(X_train_tfidf,y_train)\n",
    "    count_test = count_vectorizer.transform(X_test)\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "    print('nb: ' + str(metrics.accuracy_score(y_test,pred) * 100))\n",
    "    nbAverage += metrics.accuracy_score(y_test,pred)\n",
    "    '''\n",
    "    nn = MLPClassifier().fit(count_train, y_train)\n",
    "    predicted = nn.predict(X_new_counts)\n",
    "    print(f'NN: ',np.mean(predicted == y_test)*100)\n",
    "    nnAverage += np.mean(predicted == y_test)\n",
    "    '''\n",
    "print(\"\\nknn avg: \" + str(knnAverage/runs))\n",
    "print(\"cls avg: \" + str(clsAverage/runs))\n",
    "print(\"bg avg: \" + str(bgAverage/runs))\n",
    "print(\"nb avg: \" + str(nbAverage/runs))\n",
    "print(\"nn avg: \" + str(nnAverage/runs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefeb21-5c7d-46b4-b5db-613969826717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775550ab-2bac-49c0-8226-b8efc3399409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drwawings test \n",
    "hostVM = r''\n",
    "userVM = r''\n",
    "pwdVM = ''\n",
    "dbSQL = ''\n",
    "sqlStatement=  \"\"\"SELECT TOP (25) DrawingID, SUBSTRING(Name,9,100) as Name, DrawingTextAll\n",
    "    FROM [Generation].[quest].[Drawings]\n",
    "    WHERE DrawingTextAll IS NOT NULL AND Name LIKE '%Mill Creek%' AND DRAWINGID < \\'3437674\\' ORDER BY DrawingID DESC\n",
    "    \"\"\"\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+hostVM+';DATABASE='+dbSQL+';UID='+userVM+';PWD='+ pwdVM)\n",
    "drawings = pd.read_sql(sqlStatement, cnxn)\n",
    "drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47144780-87c8-404f-b4f4-75698a9f6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    #text_tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    text_tokens = [word for word in text_tokens if not word in stop_words]  ##Remove Stop Words\n",
    "    text_tokens = [ps.stem(word) for word in text_tokens] #Stem Words\n",
    "    text_tokens = [word for word in text_tokens if len(word) > 2] #Removing noise\n",
    "    text_tokens = [word for word in text_tokens if word.isalpha()] #Remove Punctuation\n",
    "    text_tokens = [ele for ele in text_tokens if ele.strip()] #remove empty spaces in list\n",
    "    return text_tokens\n",
    "drawings['cleanText'] = drawings['DrawingTextAll'].apply(lambda x: (processText(str(x))))\n",
    "#drawings['textCount'] = drawings['DrawingTextAll'].apply(lambda x: (count_vectorizer.transform([x])))\n",
    "drawings['textCount'] = drawings['cleanText'].apply(lambda x: (count_vectorizer.transform([ ' '.join(x)])))\n",
    "drawings['predictNB'] = drawings['textCount'].apply(lambda x: nb_classifier.predict( x ))\n",
    "drawings['predictCLS'] = drawings['textCount'].apply(lambda x: cls.predict( x ))\n",
    "drawings['predictBG'] = drawings['textCount'].apply(lambda x: bg.predict( x ))\n",
    "drawings['predictKN'] = drawings['DrawingTextAll'].apply(lambda x: text_clf.predict( [x] ))\n",
    "drawings = drawings.drop(columns = ['cleanText', 'textCount'])\n",
    "drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "7e4cfcf6-f9be-429a-bd9b-b9750ea24602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
